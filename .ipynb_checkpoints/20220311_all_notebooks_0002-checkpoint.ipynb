{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "amandeep\n",
    "start\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial import all libraries and dependencies\n",
    "import yfinance as yf\n",
    "import matplotlib.dates as mdates\n",
    "import panel as pn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from finta import TA\n",
    "# from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore wanrings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the instruments to download data of a stock/ETF .\n",
    "tickers = [\"AAPL\", \"TSLA\", \"MSFT\", \"SPY\", \"...\"] # to be decided\n",
    "\n",
    "# Fetch SPY Data from 1/1/2017 until 12/31/2021 and choosing a interval\n",
    "start_date = datetime.date(2017,1,1)\n",
    "end_date = datetime.date(2021,12,31)\n",
    "interval = '1d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pandas_reader.data.DataReader to load the desired data.\n",
    "yf.Tickers(tickers[0])\n",
    "panel_data = yf.download(tickers[0], start = start_date, end = end_date, interval = interval)\n",
    "\n",
    "# Checkout the data type\n",
    "type(panel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review data\n",
    "panel_data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data description and check if null\n",
    "def data_description(df):\n",
    "    print(\"Data Information\")\n",
    "    print(df.info())\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description(panel_data) # if 0 null and OHLC is floating and Volumne is int, then data is clean to proceed to part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ohlcv dataframe to be ready for finta\n",
    "def ohlcv(df):\n",
    "    del(df['Close'])\n",
    "    df = df.rename(columns = {\"Open\": \"open\",'High' : 'high', 'Low' : \"low\", \"Adj Close\": \"close\", 'Volume': 'volume'},inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv(panel_data)\n",
    "panel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeframe for prediction\n",
    "time_frame = [3,5,7]\n",
    "\n",
    "# Identify stock direction\n",
    "def stock_direction(df, days):# days is time frame\n",
    "    direction = (df['close'].shift(-days) > df['close'])\n",
    "    direction = direction.iloc[:-days]\n",
    "    return direction.astype(int) #return y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_direction(panel_data,time_frame[0]) # y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Finta calculate technical indicators\n",
    "# Define key window to calculate for technical analysis \n",
    "window = [5,14,21,50]\n",
    "def technical_indicators (df): # https://github.com/peerchemist/finta/blob/master/finta/finta.py\n",
    "    x = pd.DataFrame()\n",
    "    for n in range(len(window)) :  ### LOOPING DOES NOT SHOW(?)\n",
    "        a = TA.BBANDS(df,window[n])\n",
    "        a = a.add_prefix(f\"{window[n]}_\")\n",
    "        b = TA.RSI(df,window[n])\n",
    "        c = TA.PIVOT_FIB(df)\n",
    "        c = c.add_prefix(f\"{window[n]}_\")\n",
    "        d = TA.OBV(df)\n",
    "        d.rename(f\"{window[n]}_OBV\",inplace = True)\n",
    "        e = TA.SMA(df,window[n])\n",
    "        f = TA.EMA(df,window[n])\n",
    "        g = TA.ROC(df,window[n])\n",
    "        g.rename(f\"{window[n]}_ROC\",inplace = True)\n",
    "        k = TA.WILLIAMS(df,window[n])\n",
    "        temp = pd.concat([a,b,c,d,e,f,g,k],axis = 1)\n",
    "        x = pd.concat([x,temp],axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_indicators(panel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consol_data(df,days):\n",
    "    consol_data = technical_indicators(df)\n",
    "    consol_data[\"direction\"] = stock_direction(df,days)\n",
    "    consol_data.dropna(inplace = True)\n",
    "    return consol_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = consol_data(panel_data,time_frame[0])\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our features\n",
    "X = data.copy()\n",
    "X.drop(\"direction\", axis = 1, inplace = True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our target\n",
    "y = data[\"direction\"].copy()\n",
    "y.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkout the balance of our target values\n",
    "y.value_counts() # It's not that imbalance, ok to proceed further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y into X_train, X_test, y_train, y_test\n",
    "# Use 70% of the data for training and the remainder for testing\n",
    "split = int(0.7 * len(X))\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "# Fit the Standard Scaler with the training data\n",
    "# # Scale the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='linear')\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data\n",
    "##\n",
    "## the following commented out to allow compile test to continue\n",
    "##\n",
    "# classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the accuracy\n",
    "##\n",
    "## commented out to allow compile test to continue\n",
    "##\n",
    "# print(f\"Training Data Score: {classifier.score(X_train, y_train)}\")\n",
    "# print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the test data\n",
    "##\n",
    "## the following commented out to allow compile test to continue\n",
    "##\n",
    "# predictions = classifier.predict(X_test)\n",
    "# results = pd.DataFrame({\n",
    "#     \"Prediction\": predictions, \n",
    "#     \"Actual\": y_test\n",
    "# }).reset_index(drop=True)\n",
    "# results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## the following commented out to allow compile test to continue\n",
    "##\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## the following commented out to allow compile test to continue\n",
    "##\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "amandeep\n",
    "end\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "hanna\n",
    "start\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINTECH BOOTCAMP - PROJECT 2\n",
    "## Group 2 Notebook\n",
    "---\n",
    "By applying machine learning models, we examine (1) if selective technical indicators could predict the stock direction with statistically significant level (2) Which model is the best (3) Whether we could optimize the model (4) Which time frame the model could generate the best result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial import all libraries and dependencies\n",
    "import yfinance as yf\n",
    "import matplotlib.dates as mdates\n",
    "import panel as pn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "import holoviews as hv\n",
    "from finta import TA\n",
    "# from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore wanrings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "import plotly.express as px \n",
    "import plotly.io as pio\n",
    "\n",
    "from holoviews.plotting.util import process_cmap\n",
    "\n",
    "pio.renderers.default = 'notebook_connected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. DATA FETCHING AND CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the instruments to download data of a stock/ETF .\n",
    "tickers = [\"AAPL\", \"TSLA\", \"MSFT\", \"SPY\", \"...\"] # to be decided\n",
    "\n",
    "# Fetch SPY Data from 1/1/2017 until 12/31/2021 and choosing a interval\n",
    "start_date = datetime.date(2017,1,1)\n",
    "end_date = datetime.date(2021,12,31)\n",
    "interval = '1d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pandas_reader.data.DataReader to load the desired data.\n",
    "yf.Tickers(tickers[0])\n",
    "panel_data = yf.download(tickers[0], start = start_date, end = end_date, interval = interval)\n",
    "\n",
    "# Checkout the data type\n",
    "type(panel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review data\n",
    "panel_data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data description and check if null\n",
    "def data_description(df):\n",
    "    print(\"Data Information\")\n",
    "    print(df.info())\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description(panel_data) # if 0 null and OHLC is floating and Volumne is int, then data is clean to proceed to part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF data is not clean then dropping null or convert datatype\n",
    "# def data_cleaning(df):\n",
    "#     df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ohlcv dataframe to be ready for finta\n",
    "def ohlcv(df):\n",
    "    del(df['Close'])\n",
    "    df = df.rename(columns = {\"Open\": \"open\",'High' : 'high', 'Low' : \"low\", \"Adj Close\": \"close\", 'Volume': 'volume'},inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv(panel_data)\n",
    "panel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. DATA PROCESSING AND PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeframe for prediction\n",
    "time_frame = [3,5,7]\n",
    "\n",
    "# Identify stock direction\n",
    "def stock_direction(df, days):# days is time frame\n",
    "    direction = (df['close'].shift(-days) > df['close'])\n",
    "    direction = direction.iloc[:-days]\n",
    "    return direction.astype(int) #return y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_direction(panel_data,time_frame[0]) # y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Finta calculate technical indicators\n",
    "# Define key window to calculate for technical analysis \n",
    "window = [5,14,21,50]\n",
    "def technical_indicators (df): # https://github.com/peerchemist/finta/blob/master/finta/finta.py\n",
    "    x = pd.DataFrame()\n",
    "    for n in range(len(window)) :  ### LOOPING DOES NOT SHOW(?)\n",
    "        a = TA.BBANDS(df,window[n])\n",
    "        a = a.add_prefix(f\"{window[n]}_\")\n",
    "        b = TA.RSI(df,window[n])\n",
    "        c = TA.PIVOT_FIB(df)\n",
    "        c = c.add_prefix(f\"{window[n]}_\")\n",
    "        d = TA.OBV(df)\n",
    "        d.rename(f\"{window[n]}_OBV\", axis = 1, inplace = True)\n",
    "        e = TA.SMA(df,window[n])\n",
    "        f = TA.EMA(df,window[n])\n",
    "        g = TA.ROC(df,window[n])\n",
    "        g.rename(f\"{window[n]}_ROC\", axis = 1, inplace = True)\n",
    "        k = TA.WILLIAMS(df,window[n])\n",
    "        temp = pd.concat([a,b,c,d,e,f,g,k],axis = 1)\n",
    "        x = pd.concat([x,temp],axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_indicators(panel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consol_data(df,days):\n",
    "    consol_data = technical_indicators(df)\n",
    "    consol_data[\"direction\"] = stock_direction(df,days)\n",
    "    consol_data.dropna(inplace = True)\n",
    "    return consol_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = consol_data(panel_data,time_frame[0])\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our features\n",
    "X = data.copy()\n",
    "X.drop(\"direction\", axis = 1, inplace = True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our target\n",
    "y = data[\"direction\"].copy()\n",
    "y.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkout the balance of our target values\n",
    "y.value_counts() # It's not that imbalance, ok to proceed further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y into X_train, X_test, y_train, y_test\n",
    "# Use 70% of the data for training and the remainder for testing\n",
    "split = int(0.7 * len(X))\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "# Fit the Standard Scaler with the training data\n",
    "# # Scale the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. CHOOSING MODELS AND TRAINING MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model 1. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependent libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt.searchcv import BayesSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling and Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators = 250, random_state = 42)\n",
    "rf_model = rf_model.fit(X_train_scaled,y_train)\n",
    "y_train_pred = rf_model.predict(X_train_scaled)\n",
    "y_pred = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy_score is {accuracy_score(y_test,y_pred)}\")\n",
    "print(\"-\"*50)\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"-\"*50)\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC for Random Forest Classifier\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_pred,pos_label=1)\n",
    "auc_train = auc(fpr_train, tpr_train)\n",
    "auc_train = round(auc_train, 4)\n",
    "#fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred,pos_label=1)\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred)\n",
    "auc_test = auc(fpr_test, tpr_test)\n",
    "auc_test = round(auc_test, 4)\n",
    "\n",
    "# Create a DataFrame with the fpr and tpr results\n",
    "roc_df_train = pd.DataFrame({\"FPR\": fpr_train, \"TPR\": tpr_train,})\n",
    "roc_df_test = pd.DataFrame({\"FPR\": fpr_test, \"TPR\": tpr_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot roc train\n",
    "plot_train = roc_df_train.plot(\n",
    "    x=\"FPR\",\n",
    "    y=\"TPR\",\n",
    "    xlim=([-0.05, 1.05]),\n",
    "    title=f\"Train ROC Curve (AUC={auc_train})-RF\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test = roc_df_test.plot(\n",
    "    x=\"FPR\",\n",
    "    y=\"TPR\",\n",
    "    color=\"red\",\n",
    "    style=\"--\",\n",
    "    xlim=([-0.05, 1.05]),\n",
    "    title=f\"Test ROC Curve (AUC={auc_test})-RF\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the features sorted in descending order by feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "df_importances = pd.DataFrame(sorted(zip(rf_model.feature_importances_, X.columns), reverse= True))\n",
    "df_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variable importance plot\n",
    "def variable_importance(df):\n",
    "    plot = df.hvplot.barh(\"1\", stacked=True, height=1200)\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the importance level \n",
    "variable_importance(df_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPROVING AND TUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1. GridSearchCV Hyper paramater tunning\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning to determine most accurate random forest settings\n",
    "rfc_gscv=RandomForestClassifier(\n",
    "    random_state = 42, # in production remove random sseed, random seeds are only appropriate in accedemic and trainging environments\n",
    "    verbose=True\n",
    ")\n",
    "rfc_gscv_param = {\n",
    "    \"n_estimators\": [5,10,50,100,250],\n",
    "    \"max_depth\":[2,4,8,16,32,64,None],\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV model selection for cross-validation\n",
    "rfc_gs = GridSearchCV(\n",
    "    rfc_gscv,\n",
    "    rfc_gscv_param,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.fit(\n",
    "    X_train,\n",
    "    y_train.values.ravel()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cv(results_1):\n",
    "    mean_score = results_1.cv_results_['mean_test_score']\n",
    "    std_score = results_1.cv_results_['std_test_score']\n",
    "    params = results_1.cv_results_['params']\n",
    "    for mean,std,params in zip(mean_score,std_score,params):\n",
    "        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')\n",
    "    print(\"-\"*50)\n",
    "    print(f'Best parameters for Random Forest: {results_1.best_params_}')\n",
    "\n",
    "display_cv(rfc_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply best parameters from GridSearchCV to rf_2\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model based on GridSearchCV best results, manual tuning\n",
    "rf_2 = RandomForestClassifier(\n",
    "    criterion = 'gini',\n",
    "    max_depth = 2,\n",
    "    n_estimators = 10, \n",
    "    min_samples_split = 50,\n",
    "    max_features = 'log2',\n",
    "    bootstrap = True,\n",
    "    oob_score = bool,\n",
    "    random_state = 42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# fit to training data\n",
    "rf_2.fit(\n",
    "    X_train,\n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test data set used for predictions\n",
    "y_train_pred = rf_2.predict(X_train_scaled)\n",
    "y_pred = rf_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df of diffrence between actual v. predicted values\n",
    "actual_predicted_df=pd.DataFrame({\n",
    "        'Actual':y_test,\n",
    "        'Predicted':y_pred\n",
    "    }\n",
    ")\n",
    "\n",
    "actual_predicted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Evaluation: hyperparameter tuning GridSearchCV\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance as pandas series\n",
    "print(f\"Accuracy_score is {accuracy_score(y_test,y_pred)}\")\n",
    "print(\"-\"*50)\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"-\"*50)\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2. BayesSearchCV Hyper paramater tunning\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning to determine most accurate random forest settings\n",
    "rfc_bscv=RandomForestClassifier(\n",
    "    random_state = 42, # in production remove random sseed, random seeds are only appropriate in accedemic and trainging environments\n",
    "    verbose=True\n",
    ")\n",
    "rfc_bscv_param = {\n",
    "    \"n_estimators\": [5,10,50,100,250],\n",
    "    \"max_depth\":[2,4,8,16,32,64,None],\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesSearchCV model selection for cross-validation\n",
    "rfc_bs = BayesSearchCV(\n",
    "    rfc_bscv,\n",
    "    rfc_bscv_param,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_bs.fit(\n",
    "    X_train,\n",
    "    y_train.values.ravel()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cv(results_1):\n",
    "    mean_score = results_1.cv_results_['mean_test_score']\n",
    "    std_score = results_1.cv_results_['std_test_score']\n",
    "    params = results_1.cv_results_['params']\n",
    "    for mean,std,params in zip(mean_score,std_score,params):\n",
    "        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')\n",
    "    print(\"-\"*50)\n",
    "    print(f'Best parameters for Random Forest: {results_1.best_params_}')\n",
    "\n",
    "display_cv(rfc_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply best parameters from BayesSearchCV to rf_2\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model based on GridSearchCV best results, manual tuning\n",
    "rf_2 = RandomForestClassifier(\n",
    "    criterion = 'gini',\n",
    "    max_depth = 2,\n",
    "    n_estimators = 10, \n",
    "    min_samples_split = 50,\n",
    "    max_features = 'log2',\n",
    "    bootstrap = True,\n",
    "    oob_score = bool,\n",
    "    random_state = 42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# fit to training data\n",
    "rf_2.fit(\n",
    "    X_train,\n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test data set used for predictions\n",
    "y_train_pred = rf_2.predict(X_train_scaled)\n",
    "y_pred = rf_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df of diffrence between actual v. predicted values\n",
    "actual_predicted_df=pd.DataFrame({\n",
    "        'Actual':y_test,\n",
    "        'Predicted':y_pred\n",
    "    }\n",
    ")\n",
    "\n",
    "actual_predicted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Evaluation: hyperparameter tuning BayesSearchCV\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance as pandas series\n",
    "print(f\"Accuracy_score is {accuracy_score(y_test,y_pred)}\")\n",
    "print(\"-\"*50)\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"-\"*50)\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC for rf_2 Classifier\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(\n",
    "    y_train,\n",
    "    y_train_pred,\n",
    "    pos_label=1\n",
    ")\n",
    "auc_train = auc(\n",
    "    fpr_train,\n",
    "    tpr_train\n",
    ")\n",
    "auc_train = round(\n",
    "    auc_train,\n",
    "    4\n",
    ")\n",
    "\n",
    "#fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred,pos_label=1)\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(\n",
    "    y_test,\n",
    "    y_pred\n",
    ")\n",
    "auc_test = auc(\n",
    "    fpr_test,\n",
    "    tpr_test\n",
    ")\n",
    "auc_test = round(\n",
    "    auc_test,\n",
    "    4\n",
    ")\n",
    "\n",
    "# Create a DataFrame with the fpr and tpr results\n",
    "roc_df_train = pd.DataFrame(\n",
    "    {\n",
    "        \"FPR\": fpr_train,\n",
    "        \"TPR\": tpr_train\n",
    "    }\n",
    ")\n",
    "roc_df_test = pd.DataFrame(\n",
    "    {\n",
    "        \"FPR\": fpr_test,\n",
    "        \"TPR\": tpr_test\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot roc train\n",
    "plot_train_1 = roc_df_train.hvplot(\n",
    "    x=\"FPR\",\n",
    "    y=\"TPR\",\n",
    "    xlim=(\n",
    "        [\n",
    "            -0.05,\n",
    "            1.05\n",
    "        ]\n",
    "    ),\n",
    "    title=f\"Train ROC Curve (AUC={auc_train})-RF\",\n",
    ")\n",
    "\n",
    "plot_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_1 = roc_df_test.hvplot(\n",
    "    x=\"FPR\",\n",
    "    y=\"TPR\",\n",
    "    color=\"red\",\n",
    "    style=\"--\",\n",
    "    xlim=(\n",
    "        [\n",
    "            -0.05,\n",
    "            1.05\n",
    "        ]\n",
    "    ),\n",
    "    title=f\"Test ROC Curve (AUC={auc_test})-Random Forest 2_1\",\n",
    ")\n",
    "\n",
    "plot_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the features sorted in descending order by feature importance\n",
    "importances = rf_2.feature_importances_\n",
    "df_importances = pd.DataFrame(\n",
    "    sorted(\n",
    "        zip(\n",
    "            rf_2.feature_importances_,\n",
    "            X.columns\n",
    "        ),\n",
    "        reverse= True\n",
    "    )\n",
    ")\n",
    "\n",
    "df_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variable importance plot\n",
    "def variable_importance(df):\n",
    "    plot = df.hvplot.barh(\n",
    "        \"1\",\n",
    "        stacked=True,\n",
    "        height=800\n",
    "    )\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the importance level \n",
    "variable_importance(df_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Method 2. Improve the X variables quality by deleting all the insignificant features and adding more leading technical indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key window to calculate for technical analysis \n",
    "window = [5,14,21,50]\n",
    "def technical_indicators (df): # https://github.com/peerchemist/finta/blob/master/finta/finta.py\n",
    "    x = pd.DataFrame()\n",
    "    for n in range(len(window)) :  ### LOOPING DOES NOT SHOW(?)\n",
    "        #a = TA.BBANDS(df,window[n])\n",
    "        b = TA.RSI(df,window[n])\n",
    "        #c = TA.PIVOT_FIB(df) \n",
    "        #d = TA.OBV(df)\n",
    "        #e = TA.SMA(df,window[n])\n",
    "        #f = TA.EMA(df,window[n])\n",
    "        g = TA.ROC(df,window[n])\n",
    "        g.rename(f\"{window[n]}_ROC\", axis = 1, inplace = True)\n",
    "        k = TA.WILLIAMS(df,window[n])\n",
    "        l = TA.CCI(df,window[n]) # add to test\n",
    "        m = TA.DMI(df,window[n])\n",
    "        m = m.add_prefix(f\"{window[n]}_\") # DELETE n in define name\n",
    "        #o = TA.PSAR(df) #m\n",
    "        #o = o.add_prefix(f\"{window[n]}_\") # DELETE n in define name\n",
    "        temp = pd.concat([b,g,k,l,m],axis = 1) #a, c,d,e,f,m\n",
    "        x = pd.concat([x,temp],axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consol_data(df,days):\n",
    "    consol_data = technical_indicators(df)\n",
    "    consol_data[\"direction\"] = stock_direction(df,days)\n",
    "    consol_data.dropna(inplace = True)\n",
    "    return consol_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = consol_data(panel_data,time_frame[1])\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our features\n",
    "X = data.copy()\n",
    "X.drop(\"direction\", axis = 1, inplace = True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our target\n",
    "y = data[\"direction\"].copy()\n",
    "y.values.reshape(-1,1)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y into X_train, X_test, y_train, y_test\n",
    "# Use 70% of the data for training and the remainder for testing\n",
    "split = int(0.7 * len(X))\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "# Fit the Standard Scaler with the training data\n",
    "# # Scale the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators = 500, random_state = 42)\n",
    "rf_model = rf_model.fit(X_train_scaled,y_train)\n",
    "y_train_pred = rf_model.predict(X_train_scaled)\n",
    "y_pred = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy_score is {accuracy_score(y_test,y_pred)}\")\n",
    "print(\"-\"*50)\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"-\"*50)\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC for Random Forest Classifier\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_pred,pos_label=1)\n",
    "auc_train = auc(fpr_train, tpr_train)\n",
    "auc_train = round(auc_train, 4)\n",
    "#fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred,pos_label=1)\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred)\n",
    "auc_test = auc(fpr_test, tpr_test)\n",
    "auc_test = round(auc_test, 4)\n",
    "\n",
    "# Create a DataFrame with the fpr and tpr results\n",
    "roc_df_train = pd.DataFrame({\"FPR\": fpr_train, \"TPR\": tpr_train,})\n",
    "roc_df_test = pd.DataFrame({\"FPR\": fpr_test, \"TPR\": tpr_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot roc train\n",
    "plot_train_2 = roc_df_train.hvplot(\n",
    "    x=\"FPR\",\n",
    "    y=\"TPR\",\n",
    "    xlim=([-0.05, 1.05]),\n",
    "    title=f\"Train ROC Curve (AUC={auc_train})-RF\",\n",
    ")\n",
    "\n",
    "plot_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_2 = roc_df_test.hvplot(\n",
    "    x=\"FPR\",\n",
    "    y=\"TPR\",\n",
    "    color=\"red\",\n",
    "    style=\"--\",\n",
    "    xlim=([-0.05, 1.05]),\n",
    "    title=f\"Test ROC Curve (AUC={auc_test})-RF\",\n",
    ")\n",
    "\n",
    "plot_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the features sorted in descending order by feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "df_importances = pd.DataFrame(sorted(zip(rf_model.feature_importances_, X.columns), reverse= True))\n",
    "df_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variable importance plot\n",
    "def variable_importance(df):\n",
    "    plot = df.hvplot.barh(\"1\", stacked=True, height=800)\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the importance level \n",
    "variable_importance(df_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments on Random Forest Classifer on this Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The results as low as >=44 to a more consistent range ~55-59 after hyperparameter tuning shows promise, however the training model is not good. \n",
    "2. Adding more data renders higher accuracy. Although arbitrary addition of data is not in keeping with best practicies in production settings. \n",
    "3. For this pre-production/training environment random_seed=42 was applied for reproducability.\n",
    "4. While GridSearchCV was important to achieve higher accuracy scores BayesianSearchCV was test to gain insight. Results are suspect as they matched exactly to 16 decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "hanna\n",
    "end\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "nedal\n",
    "start\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINTECH BOOTCAMP - PROJECT 2\n",
    "## Group 2 Notebook\n",
    "---\n",
    "By applying machine learning models, we examine (1) if selective technical indicators could predict the stock direction with statistically significant level (2) Which model is the best (3) Whether we could optimize the model (4) Which time frame the model could generate the best result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial import all libraries and dependencies\n",
    "import yfinance as yf\n",
    "import matplotlib.dates as mdates\n",
    "import panel as pn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from finta import TA\n",
    "# from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# Ignore wanrings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. DATA FETCHING AND CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the instruments to download data of a stock/ETF .\n",
    "tickers = [\"AAPL\", \"TSLA\", \"MSFT\", \"SPY\", \"...\"] # to be decided\n",
    "\n",
    "# Fetch SPY Data from 1/1/2017 until 12/31/2021 and choosing a interval\n",
    "start_date = datetime.date(2017,1,1)\n",
    "end_date = datetime.date(2021,12,31)\n",
    "interval = '1d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pandas_reader.data.DataReader to load the desired data.\n",
    "yf.Tickers(tickers[0])\n",
    "panel_data = yf.download(tickers[0], start = start_date, end = end_date, interval = interval)\n",
    "\n",
    "# Checkout the data type\n",
    "type(panel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review data\n",
    "panel_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data description and check if null\n",
    "def data_description(df):\n",
    "    print(\"Data Information\")\n",
    "    print(df.info())\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description(panel_data) # if 0 null and OHLC is floating and Volumne is int, then data is clean to proceed to part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF data is note clean then dropping null or convert datatype\n",
    "# def data_cleaning(df):\n",
    "#     df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ohlcv dataframe to be ready for finta\n",
    "def ohlcv(df):\n",
    "    del(df['Close'])\n",
    "    df = df.rename(columns = {\"Open\": \"open\",'High' : 'high', 'Low' : \"low\", \"Adj Close\": \"close\", 'Volume': 'volume'},inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv(panel_data)\n",
    "panel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. DATA PROCESSING AND PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeframe for prediction\n",
    "time_frame = [3,5,7]\n",
    "\n",
    "# Identify stock direction\n",
    "def stock_direction(df, days):# days is time frame\n",
    "    direction = (df['close'].shift(-days) > df['close'])\n",
    "    direction = direction.iloc[:-days]\n",
    "    return direction.astype(int) #return y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_direction(panel_data,time_frame[0]) # y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Finta calculate technical indicators\n",
    "# Define key window to calculate for technical analysis \n",
    "window = [5,14,21,50]\n",
    "def technical_indicators (df): # https://github.com/peerchemist/finta/blob/master/finta/finta.py\n",
    "    x = pd.DataFrame()\n",
    "    for n in range(len(window)) :  ### LOOPING DOES NOT SHOW(?)\n",
    "        a = TA.BBANDS(df,window[n])\n",
    "        b = TA.RSI(df,window[n])\n",
    "        c = TA.PIVOT_FIB(df)\n",
    "        d = TA.OBV(df)\n",
    "        e = TA.SMA(df,window[n])\n",
    "        f = TA.EMA(df,window[n])\n",
    "        g = TA.ROC(df,window[n])\n",
    "        k = TA.WILLIAMS(df,window[n])\n",
    "        temp = pd.concat([a,b,c,d,e,f,g,k],axis = 1)\n",
    "        x = pd.concat([x,temp],axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_indicators(panel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consol_data(df,days):\n",
    "    consol_data = technical_indicators(df)\n",
    "    consol_data[\"direction\"] = stock_direction(df,days)\n",
    "    consol_data.dropna(inplace = True)\n",
    "    return consol_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = consol_data(panel_data,time_frame[0])\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "# Note: This is used for model prototyping, but it is good practice to comment this out and run multiple experiments to evaluate your model.\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(1)\n",
    "from tensorflow import random\n",
    "\n",
    "random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_data(df, window, feature_col_number, target_col_number):\n",
    "    \"\"\"\n",
    "    This function accepts the column number for the features (X) and the target (y).\n",
    "    It chunks the data up with a rolling window of Xt - window to predict Xt.\n",
    "    It returns two numpy arrays of X and y.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df) - window):\n",
    "        features = df.iloc[i : (i + window), feature_col_number]\n",
    "        target = df.iloc[(i + window), target_col_number]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the features (X) and target (y) data using the window_data() function.\n",
    "window_size = 30\n",
    "# parsing the most important featurs (S1,S4,21 period SMA,14 perioid RSI,BB LOWER,ROC) \n",
    "feature_column = [8,26,44,62,50,21,2,20,38,56,16,34,52,5,23,41,59]\n",
    "target_column =[72]\n",
    "X, y = window_data(data, window_size, feature_column, target_column)\n",
    "print (f\"X sample values:\\n{X[:5]} \\n\")\n",
    "print (f\"y sample values:\\n{y[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 70% of the data for training and the remaineder for testing\n",
    "split=int(0.7 * len(X))\n",
    "X_train=X[: split]\n",
    "X_test=X[split :]\n",
    "y_train=y[: split]\n",
    "y_test=y[split :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the X and y to be in 2d array as the  MinMaxScaler only accept 2d arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_train.shape\n",
    "X_train = X_train.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_test.shape\n",
    "X_test=X_test.reshape((nsamples,nx*ny))\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why should we scale target variables in regression problems?\n",
    "\n",
    "###### A target variable with a large spread of values, in turn, may result in large error gradient values causing weight values to change dramatically, making the learning process unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the MinMaxScaler to scale data between 0 and 1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the MinMaxScaler object with the training feature data X_train\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Scale the features training and testing sets\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fit the MinMaxScaler object with the training target data y_train\n",
    "scaler.fit(y_train)\n",
    "\n",
    "# Scale the target training and testing sets\n",
    "y_train = scaler.transform(y_train)\n",
    "y_test = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Dimensions Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce dimensions to 3 principal components\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "X_train= pca.fit_transform(X_train)\n",
    "X_test=pca.transform(X_test)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Features Data for the LSTM Model\n",
    "\n",
    "The LSTM API from Keras needs to receive the features data as a _vertical vector_, so that we need to reshape the `X` data in the form `reshape((X_train.shape[0], X_train.shape[1], 1))`.\n",
    "\n",
    "Both sets, training, and testing are reshaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the features for the model\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "print (f\"X_train sample values:\\n{X_train[:5]} \\n\")\n",
    "print (f\"X_test sample values:\\n{X_test[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train the LSTM RNN\n",
    "\n",
    "In this section, we will design a custom LSTM RNN in Keras and fit (train) it using the training data we defined.\n",
    "\n",
    "we we need to:\n",
    "\n",
    "1. Define the model architecture in Keras.\n",
    "\n",
    "2. Compile the model.\n",
    "\n",
    "3. Fit the model with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Keras modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Dropout`: Dropout is a regularization technique for reducing overfitting in neural networks. This type of layer applies the dropout technique to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM RNN model.\n",
    "model = Sequential()\n",
    "\n",
    "# Initial model setup\n",
    "number_units = 30\n",
    "dropout_fraction = 0.5\n",
    "\n",
    "# Layer 1\n",
    "model.add(LSTM(\n",
    "    units=number_units,\n",
    "    return_sequences=True,\n",
    "    input_shape=(X_train.shape[1], 1)))\n",
    "    \n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 2\n",
    "model.add(LSTM(units=number_units, return_sequences=True))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 3\n",
    "model.add(LSTM(units=number_units))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the LSTM RNN Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", metrics=\"accuracy\", loss = \"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='accuracy',\n",
    "                               patience=10,\n",
    "                               mode='max',\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train ,y_train, epochs=100, shuffle=False, batch_size=16, verbose=1,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the testing data X_test\n",
    "predicted = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat DataFrame for Predicted Vs. Real Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score\n",
    "from IPython.display import display\n",
    "\n",
    "THRESHOLD = [0.4,0.5,0.52 ,0.55, 0.57, 0.58 ,0.6,0.7,0.8]\n",
    "for i in THRESHOLD:\n",
    "    preds = np.where(model.predict(X_test).ravel() > i, 1, 0)\n",
    "    print(i)\n",
    "    df_thresh = pd.DataFrame(data=[accuracy_score(y_test, preds), recall_score(y_test, preds),\n",
    "                       precision_score(y_test, preds), roc_auc_score(y_test, preds)], \n",
    "                 index=[\"accuracy\", \"recall\", \"precision\", \"roc_auc_score\"], columns = [\"Scores\"])\n",
    "    display(df_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "predicted = np.where(predicted > THRESHOLD ,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of Real and Predicted values\n",
    "stocks = pd.DataFrame({\n",
    "    \"Actual\": y_test.ravel(),\n",
    "    \"Predicted\": preds.ravel()\n",
    "}, index = data.index[-len(y_test): ]) \n",
    "\n",
    "# Show the DataFrame's head\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "print(classification_report(y_test,predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot  roc_curve and auc metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the roc_curve and auc metrics from sklearn\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions to feed the roc_curve module\n",
    "train_predictions = model.predict(X_train, batch_size=1000)\n",
    "test_predictions = model.predict(X_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ROC curve and AUC for the training set\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, train_predictions)\n",
    "auc_train = auc(fpr_train, tpr_train)\n",
    "auc_train = round(auc_train, 4)\n",
    "\n",
    "# Calculate the ROC curve and AUC for the testing set\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, test_predictions)\n",
    "auc_test = auc(fpr_test, tpr_test)\n",
    "auc_test = round(auc_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the fpr and tpr results\n",
    "roc_df_train = pd.DataFrame({\"FPR Train\": fpr_train, \"TPR Train\": tpr_train,})\n",
    "\n",
    "roc_df_test = pd.DataFrame({\"FPR Test\": fpr_test, \"TPR Test\": tpr_test,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC Curves\n",
    "roc_df_train.hvplot(\n",
    "    x=\"FPR Train\",\n",
    "    y=\"TPR Train\",\n",
    "    xlim=([-0.05, 1.05]),\n",
    "    title=f\"Train ROC Curve (AUC={auc_train})\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_df_test.hvplot(\n",
    "    x=\"FPR Test\",\n",
    "    y=\"TPR Test\",\n",
    "    color=\"red\",\n",
    "    style=\"--\",\n",
    "    xlim=([-0.05, 1.05]),\n",
    "    title=f\"Test ROC Curve (AUC={auc_test})\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "nedal\n",
    "end\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "nimai\n",
    "start\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINTECH BOOTCAMP - PROJECT 2\n",
    "## Group 2 Notebook\n",
    "---\n",
    "By applying machine learning models, we examine (1) if selective technical indicators could predict the stock direction with statistically significant level (2) Which model is the best (3) Whether we could optimize the model (4) Which time frame the model could generate the best result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial import all libraries and dependencies\n",
    "import yfinance as yf\n",
    "import matplotlib.dates as mdates\n",
    "import panel as pn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from finta import TA\n",
    "# from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore wanrings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. DATA FETCHING AND CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the instruments to download data of a stock/ETF .\n",
    "tickers = [\"AAPL\", \"TSLA\", \"MSFT\", \"SPY\", \"...\"] # to be decided\n",
    "\n",
    "# Fetch SPY Data from 1/1/2017 until 12/31/2021 and choosing a interval\n",
    "start_date = datetime.date(2017,1,1)\n",
    "end_date = datetime.date(2021,12,31)\n",
    "interval = '1d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pandas_reader.data.DataReader to load the desired data.\n",
    "yf.Tickers(tickers[0])\n",
    "panel_data = yf.download(tickers[0], start = start_date, end = end_date, interval = interval)\n",
    "\n",
    "# Checkout the data type\n",
    "type(panel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review data\n",
    "panel_data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data description and check if null\n",
    "def data_description(df):\n",
    "    print(\"Data Information\")\n",
    "    print(df.info())\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description(panel_data) # if 0 null and OHLC is floating and Volumne is int, then data is clean to proceed to part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF data is not clean then dropping null or convert datatype\n",
    "# def data_cleaning(df):\n",
    "#     df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ohlcv dataframe to be ready for finta\n",
    "def ohlcv(df):\n",
    "    del(df['Close'])\n",
    "    df = df.rename(columns = {\"Open\": \"open\",'High' : 'high', 'Low' : \"low\", \"Adj Close\": \"close\", 'Volume': 'volume'},inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv(panel_data)\n",
    "panel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. DATA PROCESSING AND PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeframe for prediction\n",
    "time_frame = [3,5,7]\n",
    "\n",
    "# Identify stock direction\n",
    "def stock_direction(df, days):# days is time frame\n",
    "    direction = (df['close'].shift(-days) > df['close'])\n",
    "    direction = direction.iloc[:-days]\n",
    "    return direction.astype(int) #return y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_direction(panel_data,time_frame[0]) # y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Finta calculate technical indicators\n",
    "# Define key window to calculate for technical analysis \n",
    "window = [5,14,21,50]\n",
    "def technical_indicators (df): # https://github.com/peerchemist/finta/blob/master/finta/finta.py\n",
    "    x = pd.DataFrame()\n",
    "    for n in range(len(window)) :  ### LOOPING DOES NOT SHOW(?)\n",
    "        a = TA.BBANDS(df,window[n])\n",
    "        a = a.add_prefix(f\"{window[n]}_\")\n",
    "        b = TA.RSI(df,window[n])\n",
    "        c = TA.PIVOT_FIB(df)\n",
    "        c = c.add_prefix(f\"{window[n]}_\")\n",
    "        d = TA.OBV(df)\n",
    "        d.rename(f\"{window[n]}_OBV\", axis = 1, inplace = True)\n",
    "        e = TA.SMA(df,window[n])\n",
    "        f = TA.EMA(df,window[n])\n",
    "        g = TA.ROC(df,window[n])\n",
    "        g.rename(f\"{window[n]}_ROC\", axis = 1, inplace = True)\n",
    "        k = TA.WILLIAMS(df,window[n])\n",
    "        temp = pd.concat([a,b,c,d,e,f,g,k],axis = 1)\n",
    "        x = pd.concat([x,temp],axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_indicators(panel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consol_data(df,days):\n",
    "    consol_data = technical_indicators(df)\n",
    "    consol_data[\"direction\"] = stock_direction(df,days)\n",
    "    consol_data.dropna(inplace = True)\n",
    "    return consol_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = consol_data(panel_data,time_frame[0])\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our features\n",
    "X = data.copy()\n",
    "X.drop(\"direction\", axis = 1, inplace = True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our target\n",
    "y = data[\"direction\"].copy()\n",
    "y.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkout the balance of our target values\n",
    "y.value_counts() # It's not that imbalance, ok to proceed further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y into X_train, X_test, y_train, y_test\n",
    "# Use 70% of the data for training and the remainder for testing\n",
    "split = int(0.7 * len(X))\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "# Fit the Standard Scaler with the training data\n",
    "# # Scale the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. CHOOSING MODELS AND TRAINING MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependent libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model is for Logisitic Regression and Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='lbfgs', random_state=42)\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate accuracy score\n",
    "y_pred_2 = model.predict(X_test_scaled)\n",
    "print(accuracy_score(y_test, y_pred_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display confusion matrix\n",
    "confusion_matrix(y_test, y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print classification report\n",
    "print(classification_report(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing learning rates\n",
    "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "for learning_rate in learning_rates:\n",
    "    model_2 = GradientBoostingClassifier(\n",
    "        #criterion='gini',\n",
    "        max_depth=2,\n",
    "        n_estimators=10,\n",
    "        min_samples_split=50,\n",
    "        max_features='log2',\n",
    "        #bootstrap=True,\n",
    "        #oob_score=bool,\n",
    "        random_state=42,\n",
    "        verbose=True\n",
    "    )\n",
    "    model_2.fit(X_train_scaled, y_train.ravel())\n",
    "    print('Learning Rate: ', learning_rate)\n",
    "\n",
    "    #Score the model\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(\n",
    "        model_2.score(\n",
    "            X_train_scaled,\n",
    "            y_train.ravel())))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(\n",
    "        model_2.score(\n",
    "            X_test_scaled,\n",
    "            y_test.ravel())))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model using grading boosting classifier\n",
    "model_2 = GradientBoostingClassifier(\n",
    "        #criterion='gini',\n",
    "        max_depth=2,\n",
    "        n_estimators=10,\n",
    "        min_samples_split=50,\n",
    "        max_features='log2',\n",
    "        #bootstrap=True,\n",
    "        #oob_score=bool,\n",
    "        random_state=42,\n",
    "        verbose=True)\n",
    "#fitting the model\n",
    "model_2.fit(X_train_scaled, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "predictions =model_2.predict(X_test_scaled)\n",
    "predictions_train = model_2.predict(X_train_scaled)\n",
    "#Generating accuracy score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model using confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1']\n",
    ")\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating classification report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the Gradient boosting Classifier\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, predictions_train, pos_label=1)\n",
    "auc_train = auc(fpr_train, tpr_train)\n",
    "auc_train = round(auc_train, 4)\n",
    "#fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred,pos_label=1)\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, predictions)\n",
    "auc_test = auc(fpr_test, tpr_test)\n",
    "auc_test = round(auc_test, 4)\n",
    "\n",
    "# Create a DataFrame with the fpr and tpr results\n",
    "roc_df_train = pd.DataFrame({\"FPR\": fpr_train, \"TPR\": tpr_train})\n",
    "roc_df_test_GBC = pd.DataFrame({\"FPR\": fpr_test, \"TPR\": tpr_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "roc_df_train.hvplot(\n",
    "    x=\"FPR\",\n",
    "    y=\"TPR\",\n",
    "    xlim=([-0.05, 1.05]),\n",
    "    title=f\"Train ROC Curve (AUC={auc_train})-RF\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the test\n",
    "roc_df_test_GBC.hvplot(\n",
    "    x='FPR',\n",
    "    y='TPR', \n",
    "    style='--',\n",
    "    color='red',\n",
    "    xlim=([-0.05, 1.05]),\n",
    "    title=f'Test ROC Curve (AUC={auc_test})-RF',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "nimai\n",
    "end\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. ANALYSIS AND EVALUATION (TEAM WORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. DEPLOYING MODEL (TEAM WORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. CONCLUSION (TEAM WORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47d1af71198b45214dcc5aa09cc344af3494b622a386be63e8ac528adb4b0f0a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
